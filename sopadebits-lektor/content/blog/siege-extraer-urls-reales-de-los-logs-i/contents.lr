---
_model: blog-post
---
title: Siege: extraer URLs reales de los logs (I)
---
date: 2013-06-03 10:30:01 +0200
---
author: Mario Alberich
---
categories: informatica
---
tags: linux,testeo
---
body:

<p><a title="Siege Home" href="http://www.joedog.org/siege-home/" target="_blank">Siege</a> es una herramienta de pruebas de carga, que permite simular el comportamiento de una web bajo condiciones extremas (el asedio del servidor).</p><!--more-->
<p>Un número de peticiones altas, fuerte nivel de concurrencia, etc., son circunstancias a las que se enfrentará una web con una audiencia elevada, o a puntas de consultas, como por ejemplo debido a campa&ntilde;as publicitarias, eventos específicos, salida a la venta de nuevos productos, y otros.</p>
<p>Así que nos encontramos en esa situación, y decidimos que la herramienta es siege. &nbsp;&iquest;qué más hay que hacer?</p>
<p>En ubuntu/debian, la instalación es sencilla, desde una terminal:</p>
<pre class="brush: bash; gutter: true">~$ sudo apt-get install siege</pre><br />
Con eso ya tendremos instalado siege, y podremos ejecutarlo. &nbsp;Podemos ejecutar una prueba, contra la web que queremos testear:</p>
<pre class="brush: bash; gutter: true">~$ siege http://{host}/</pre><br />
o también:</p>
<pre class="brush: bash; gutter: true">~$ siege -c 30 http://{host}/</pre><br />
para simular 30 usuarios concurrentes.</p>
<p>Donde {host} es el nombre del dominio que se quiere probar. Obvia decir que esta prueba debe realizarse en un <strong>entorno controlado y bajo nuestra propia responsabilidad</strong>, y que en ningún caso se debe ejecutar sobre servidores y webs ajenas. Las consecuencias de ese uso impropio quedan bajo la responsabilidad del usuario.</p>
<h2>Paso 1: Obtener las direcciones</h2><br />
Bien, si con lo anterior ya se puede realizar la prueba, &iquest;Para qué necesitamos más? Pues porque las simulaciones pueden requerir una variedad mayor de URLs, que cargan diversos componentes, tienen estrategias diversas de caché, o por lo que sea.</p>
<p>Y &iquest;de dónde sacamos esas URLs? Una de las opciones es un archivo de logs de Apache. &nbsp;Si dispones de un log de apache con datos de navegación real, o se puede generar uno visitando la web. &nbsp;Por ejemplo, se puede programar un crawler sencillo, tal como <a title="Httrack" href="http://www.httrack.com/" target="_blank">Httrack</a> o similar. Lo que interesa no es la descarga de contenidos, sino que se navegue por la web para generar logs de Apache.</p>
<h2>Paso 2: Limpiar y preparar las URLs</h2><br />
El log de apache tiene un formato similar al siguiente:</p>
<pre class="brush: text; gutter: true">189.147.178.214 - - [19/May/2013:06:41:18 +0200] "GET /wp-content/themes/retina/style.css HTTP/1.1" 200 5213 "http://www.sopadebits.com/patrones-organizativos-gestion-informacion" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.64 Safari/537.31"</pre><br />
Podemos identificar:</p>
<ul>
<li>Dirección IP</li>
<li>Fecha y hora de acceso</li>
<li>Método de acceso (GET), URI (sin dominio) y versión del protocolo</li>
<li>resultado de la petición (200 = Ok) y tama&ntilde;o del contenido retornado (5213 bytes)</li>
<li>URL desde donde se ha llegado (el referer)</li>
<li>Firma del navegador, y sistema operativo.</li><br />
</ul><br />
Lo que nos interesa es la URI, a la que posteriormente deberemos a&ntilde;adir el dominio completo.</p>
<p>&iquest;Cómo extraeremos esa URL? Con <a title="GNU's AWK guide" href="http://www.gnu.org/software/gawk/manual/gawk.html" target="_blank">awk</a>. Esto permite extraer los contenidos separándolos por espacios (entre infinidad de otras posibilidades, vale la pena echarle un ojo). &nbsp;En nuestro caso, la instrucción en consola puede ser:</p>
<pre class="brush: bash; gutter: true">~$ cat {archivo-de-log} | awk &#039;{print $7}&#039;</pre><br />
Bien, eso genera un listado considerable de URLs, muchas de ellas repetidas. Las ordenaremos y obtendremos la lista única de URIs:</p>
<pre class="brush: bash; gutter: true">~$ cat {archivo-de-log} | awk &#039;{print $7}&#039; | sort | uniq</pre><br />
Voil&agrave;! Aquí tienes una lista de URIs únicas, casi ya lo tenemos. Sólo queda a&ntilde;adir el protocolo y el nombre del host. Se puede conseguir echando mano del comando sed:</p>
<pre class="brush: bash; gutter: true">~$ cat {archivo-de-log} | awk &#039;{print $7}&#039; | sort | uniq | sed -e &#039;s/^/http:\/\/sopadebits/g&#039;</pre><br />
Qué hace esto exactamente? Busca el inicio de la cadena (el carácter "^" es una expresión regular que indica eso) y le a&ntilde;ade el dominio (en mi caso, el de este blog sin el ".com"). Es necesario&nbsp;<em>escapar</em> las barras (\/), como ya es habitual en las expresiones regulares.</p>

---