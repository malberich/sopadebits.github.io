---
layout: post
status: publish
published: true
title: "#MapReduce: probar en #linux antes de ejecutar en #Hadoop"
author:
  display_name: Mario Alberich
  login: admin
  email: malberich@gmail.com
  url: ''
author_login: admin
author_email: malberich@gmail.com
wordpress_id: 27845
wordpress_url: http://www.sopadebits.com/?p=27845
date: '2014-04-29 10:30:33 +0200'
date_gmt: '2014-04-29 08:30:33 +0200'
categories:
- Matemática
- Informática
tags:
- Linux
- mapreduce
- pipes
comments: []
---
<h2>Diez a&ntilde;os de MapReduce</h2><br />
En Diciembre se cumplirán diez a&ntilde;os desde que <a href="http://research.google.com/archive/mapreduce.html">Google publicó el&nbsp;<em>paper</em> sobre MapReduce</a>. El objetivo de ese artículo era exponer un algoritmo para procesar paralelamente grandes cantidades de datos utilizando una infraestructura basada en equipos informáticos modestos, y que por ello fuera más fácilmente escalable.</p><!--more-->
<!--more-->
<p>Diez a&ntilde;os más tarde MapReduce se encuentra en el núcleo del BigData, especialmente a través de Hadoop. &nbsp;Todas o casi <a title="16 Top Big Data Analytics Platforms" href="http://www.informationweek.com/big-data/big-data-analytics/16-top-big-data-analytics-platforms/d/d-id/1113609">todas las grandes compa&ntilde;ías que ofertan bigdata</a> ofrecen algún tipo de implementación de Hadoop a sus clientes.</p>
<p>Pero Hadoop es una implementación que no sólo incluye un algoritmo, sino también un sistema de archivos (Hadoop Filesystem, HDFS, basado a su vez en el Google File System, GFS) y todos los mecanismos de control necesario para llevar a cabo el tratamiento de datos a gran escala.</p>
<p>Entonces, si lo que queremos es entender el cambio de paradigma que supone MapReduce para tratar datos, &iquest;Necesitamos instalar Hadoop o un similar para entender el funcionamiento de MapReduce? Si lo que quieres probar son tus scripts con un peque&ntilde;o conjunto de datos, no.</p>
<h2>Consola y pipes</h2><br />
MapReduce tiene como base de funcionamiento el traspaso de información entre los procesos mediante&nbsp;<em>pipes</em>,&nbsp;lo que permite evitar el almacenamiento de datos en la memoria del proceso. Por ello, los scripts deben basarse en capturar la información del STDIN, y retornar la información a través del STDOUT. Si por ejemplo los datos están en un archivo CSV, podemos volcar el contenido con el comando&nbsp;<em>cat</em> (o <em>head</em>, o&nbsp;<em>tail</em> para sólo enviar un subconjunto de datos), enviándolo con un pipe a nuestro script de mapeo:</p>
<pre class="brush: bash; gutter: true">cat archivo.csv | ./mapper.php</pre><br />
Entonces, el script mapper.php debería <em>abrir</em>&nbsp;el flujo de datos recibidos a través de STDIN para empezar a procesarlo:</p>
<pre class="brush: php; gutter: true">#!/usr/bin/php<br />
<?php<br />
$stdin = fopen( &#039;php://stdin&#039;, &#039;r&#039; );</p>
<p>while( $line = fgetcsv( $stdin, 10000) ) {<br />
// Procesar/mapear los datos y ejecutar un echo sprintf:<br />
  echo sprintf("%s\n", $line[0]);<br />
}<br />
?></pre><br />
No a&ntilde;ado ningún tipo de control en el script anterior, para ponerlo lo más simple posible. El script anterior imprime el contenido de la primera columna del archivo CSV.</p>
<p>En el caso del script de reducer (reducer.php), lo que haremos es contar cuántas veces aparece cada cadena en el listado (es decir que ignoraremos el valor numérico). &nbsp;El script es bastante similar al del mapper, pero ahora mantendremos un contador de las veces que aparece la clave, y cuando ésta cambie, la imprimiremos:</p>
<pre class="brush: actionscript3; gutter: true">#!/usr/bin/php<br />
<?php<br />
$stdin = fopen( &#039;php://stdin&#039;, &#039;r&#039; );<br />
$currentKey   = null;<br />
$currentCount = 0;<br />
while( $line = fgets( $stdin ) ) {<br />
// Generas el proceso de &#039;reducción&#039;<br />
  if ($line !== $currentKey) {<br />
    echo sprintf("%s\t%s\n", $currentKey, $currentCount);<br />
    $currentKey = $fields[0];<br />
    $currentCount = 0;<br />
  }<br />
  $currentCount++;<br />
}<br />
if ($currentKey !== null) {<br />
 echo sprintf("%s\t%s", $currentKey, $currentCount);<br />
}<br />
?></pre><br />
Vale, entonces ya tienes mapper.php y reducer.php. Para simplificar mucho, sólo queda un paso intermedio entre el mapper y el reducer: la ordenación de los registros recibidos (la parte <em>Sort</em> de&nbsp;lo que se denomina el <a href="https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-6/shuffle-and-sort"><em>Shuffle and Sort</em></a> del algoritmo). Para que el reducer pueda calcular las veces que aparece cada clave, debe recibir los datos ordenados según esta clave. Para el caso que nos ocupa, lo podemos conseguir con el comando&nbsp;<em>sort</em> de Linux, no hace falta que nos compliquemos más.</p>
<h2>Ejecutando el proceso conjunto</h2><br />
Un paso más antes de probar: para poder ejecutar los dos scripts php desde la línea de comandos es necesario marcarlos como ejecutables:</p>
<pre class="brush: bash; gutter: true">chmod +x mapper.php reducer.php</pre><br />
Todo a punto. &nbsp;Poniéndolo todo en orden, tenemos que el comando sería:</p>
<pre class="brush: actionscript3; gutter: true">cat archivo.csv | ./mapper.php | sort | ./reducer.php</pre><br />
Y, reducer.php debería generar una salida con los datos procesados (en este caso, un contador de la frecuencia de cada clave). Así que a grandes rasgos, esto es lo que plantea MapReduce: un algoritmo que pueda recibir una entrada de datos y que pueda procesarla, de forma independiente al resto del dataset.</p>
<p>El resto de la implementación de MapReduce (replicación de datos, gestión de recursos, partición del dataset, etc.) es imprescindible para operar con un gran conjunto de datos, pero no para entender el concepto de base: procesar datos de forma paralela.</p>
<p><strong>Nota final</strong>: Aunque Hadoop requiere el uso de Java para desarrollar trabajos MapReduce, incorpora también una funcionalidad llamada&nbsp;<a title="Apache - Hadoop Streaming" href="http://hadoop.apache.org/docs/r1.2.1/streaming.html"><em>Hadoop Streaming</em></a> que permite la ejecución de scripts MapReduce en prácticamente cualquier lenguaje de programación. La contrapartida es un mayor uso de recursos y mayor lentitud, pero desde luego ayuda a realizar pruebas sencillas con scripts de PHP, python o similares.</p>
