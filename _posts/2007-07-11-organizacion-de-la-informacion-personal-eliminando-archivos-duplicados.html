---
layout: post
status: publish
published: true
title: "Organización de la información personal - I: eliminando
  archivos duplicados"
author:
  display_name: Mario Alberich
  login: admin
  email: malberich@gmail.com
  url: ''
author_login: admin
author_email: malberich@gmail.com
excerpt: "<p>Los archivos duplicados son un tipo muy concreto de archivos inútiles
  que, al menos en mi caso, aparecen normalmente cuando no se tiene un criterio claro
  a la hora de organizar la información o bien cuando se ha cambiado de criterio
  (y no se ha hecho una revisión completa de lo que hay hasta la fecha). </p><!--more--><p>Los
  archivos duplicados son de los primeros tipos de archivos eliminables que he abordado,
  debido a que son los más prescindibles (si me equivoco al borrar, tengo otro
  idéntico). De este modo, se inicia el proceso de limpieza reduciendo lo claramente
  inútil y por lo tanto disminuyendo la lista de archivos a verificar y reorganizar.</p><p>Durante
  las últimas semanas, aprovechando un relativo descenso de actividad (combinado
  con una necesidad imperante por reorganizar y limpiar mis discos duros) he estado
  buscando programas para que hagan la tarea sucia de identificar archivos duplicados...
  </p>"
wordpress_id: 26768
date: '2007-07-11 00:00:00 +0200'
date_gmt: '2007-07-11 00:00:00 +0200'
categories:
- Documentación
tags:
- gestión documental
- utilidades
comments: []
---
<p><br />
<h2>Consideraciones previas</h2>
<p>Una de las primera cosas que debo decir antes de entrar en detalle, es que la detección de duplicados no debe conducir al borrado indiscriminado de las copias aparentemente inútiles. Por poner mi caso, el proceso de borrado se ha limitado a documentos, hojas de cálculo y demás archivos de trabajo. Por lo tanto, no he intentado eliminar programas, bibliotecas dinámicas (SO en linux, DLL en Windows) ni otras utilidades. Me he centrado en archivos pasivos, que no forman parte de procesos ni programas, cosa que recomiendo a todo aquel que no tenga claras las consecuencias de borrar lo que no debe borrarse. Para eso existen <a href="http://www.webmasterfree.com/RegCleaner_d7625.html" title="Regcleaner">otras herramientas</a> y probablemente <a href="http://www.dummies.com/WileyCDA/DummiesArticle/id-2635.html" title="dummies: cleaning windows XP">otros tutoriales</a>.</p>
<p>En segundo término, es importante aclarar que aunque se traten de dos documentos idénticos, hay que tener en cuenta el por qué se borran unos u otros, y si es necesario. Al revés del refrán, vale más la pena tener cien documentos duplicados a mano que uno volando. Eso siempre que tengas claro qué es lo bueno y qué es lo malo. </p>
<p>Una vez dicho eso, sólo quiero a&ntilde;adir que mi uso más intensivo se ha centrado a las utilidades de Linux, aunque he echado un vistazo y he testeado una utilidad en Windows sin incidencias remarcables.</p><br />
<h2>Conceptos base para la detección de duplicados</h2><br />
<h3>LA gestión de archivos: Unix y Windows<br />  </h3>
<p>Existen varias utilidades posibles para la detección y eliminación de duplicados. Sólo hay que pensar que UNIX es un sistema operativo más pensado desde sus inicios para el uso en grandes servidores, con gran cantidad de datos y por lo tanto con más dificultades en la gestión y administración de sistemas. </p>
<p>Dentro de esta gestión de sistemas se encuentran las utilidades de copia de seguridad, integridad del sistema de archivos y demás. La detección de archivos duplicados es una tarea accesoria que se puede realizar puntualmente, pero que desde luego toma su tiempo llevar a cabo sin las herramientas básicas.</p>
<p>Windows, por su lado, ha ido mejorando la estructura de directorios para poder diferenciar las ubicaciones de los archivos de programas y los documentos de trabajo del usuario.  Sin que se haya conseguido concienciar más a una parte de usuarios que se obcecan en montar murales de documentos en su escritorio, lo cierto es que algo han conseguido.  Por otro lado, las utilidades de gestión del sistema se han ido introduciendo con el tiempo. Me reservo mi opinión, porque entre otras cosas, no me considero un experto en sistemas, y mucho menos de Windows.</p><br />
<h3>Optimización del proceso de comparación<br /> </h3>
<p>Comparar dos archivos byte a byte sería una verdadera locura.  Encontrar archivos duplicados en una lista de 10000 implicaría efectuar hasta (10000*9.999)/2=49.995.000 de revisiones en los archivos, lo que es computacionalmente muy ineficiente. </p>
<p>También es cierto que cuando se identifica un duplicado nos ahorramos recorrer tantas veces los archivos, pero en el caso que no hubiera ni un solo archivo idéntico en todo el sistema, recorrerías todas esas veces los archivos.  Al final del proceso, además de tener claro que no tienes duplicados, quizá también debas plantearte un cambio de disco duro... Vamos, que no van por ahí los tiros.</p>
<p>Para identificar la duplicidad de archivos, el método pasa por establecer algoritmos que de generen una clave única de tales archivos, y así pueda compararse archivo por archivo, si las claves coinciden.  Este es un procedimiento mucho más eficiente que el anterior, porque como máximo hay que recorrer 10000 veces los archivos (lo cual reduce en 9999 veces el volumen máximo de tareas a realizar).</p>
<p>A pesar de esto, es probable que en la mayoría de casos con leer peque&ntilde;os bloques de unos cuantos bytes de un archivo ya se pueda descartar candidatos a duplicados, con lo que aún nos ahorramos el recorrer gran parte de los archivos (y eso es importante si los archivos son grandes). </p>
<p>Como se ve, existen varios trucos basados en el <strong>muestreo</strong> que permiten agilizar el proceso sin perder coherencia en el proceso. Como siempre que se realiza un proceso de muestreo, lo importante es la representatividad y no el tama&ntilde;o de la muestra. Si con un byte tuviéramos suficiente, no sería necesario coger dos. </p><br />
<h3>Algoritmos de generación de claves de resumen<br />  </h3>
<p>Los métodos básicos para la detección de archivos duplicados es el cálculo de las claves de tipo resumen (o <em>digest</em>), como por ejemplo los algoritmos <a href="http://www.faqs.org/rfcs/rfc1321.html" title="MD5">MD5</a>, ó <a href="http://www.w3.org/PICS/DSig/SHA1_1_0.html" title="W3C: SHA1">SHA1</a>. Son casos concretos y hay bastantes más. El más utilizado es el MD5, especialmente debido a su popularidad y grado de implantación. Aunque ya no sirve como algoritmo criptográfico, es más que suficiente para la detección e indexación fiable de archivos.  </p>
<p>Las claves de tipo <em>digest</em> asignan un valor de clave en base al contenido del archivo. Esta clave, a efectos prácticos, es única.  Digo que lo es a <em>efectos prácticos</em> porque aunque se han detectado coincidencias (lo que se llama comúnmente <em>colisiones</em>) en la generación de claves, es prácticamente imposible que dos archivos coincidan en su clave. Por ejemplo, la clave MD5 consta de 32 dígitos hexadecimales (valores 0..F): considerando que se trata de variaciones con repetición, nos da un total de 1.208925819614629175e+24 combinaciones posibles, si no me equivoco (no, no lo he hecho mentalmente...).</p>
<p>Un ejemplo de clave md5 sería: 5a948ab8f80483be4e495c8c8fba39cd </p>
<p>Si tuvieras 1 millón de archivos a comparar, existiría una probabilidad de 1 entre 10<sup>18</sup> de que dos de tus archivos tuvieran esta misma clave MD5, sin que fueran idénticos en contenido. La probabilidad es teórica (si el algoritmo fuera perfecto), pero en cualquier caso creo que quedan claras las proporciones, así que doy por hecho que entiendes que es difícil que dos archivos diferentes pasen por idénticos, a no ser que esa sea tu intención. </p>
<p>&iquest;Y qué haces si te sucede? Bueno... pues dado lo poco probable del asunto, y que la máquina ya te reduce el trabajo, siempre puedes abrir los dos archivos y comparar.  Pero vamos, yo he hecho el tratamiento sobre más de 150.000 archivos y no ha habido errores de este tipo. </p><br />
<h2>Herramientas de búsqueda de duplicados en Linux</h2>
<p>En Linux he testeado dos: la primera, llamada <a href="http://www.pixelbeat.org/fslint/" title="Fslint">fslint</a>, es una utilidad a nivel de escritorio que además de detectar duplicados, permite identificar otros problemas del sistema de archivos (datos colgando, partes corruptas del disco, etc). </p>
<p>Por mi familiaridad con la línea de comandos, descarté fslint en el momento de conocer de la existencia de <a href="http://en.wikipedia.org/wiki/Fdupes" title="Wikipedia: fdupes">fdupes</a>, aunque si tenéis más interés en utilizar fslint os recomiendo que lo utilicés para hacer revisiones de  volúmenes de datos no muy grandes: si se pasa mucho rato comparando, a veces se queda colgado.</p><br />
<blockquote> :~# fdupes -r . > duplicados.txt <br />Progress [0/154304] 0% </blockquote>
<p>El resultado de fdupes se guarda en el archivo duplicados.txt, que contiene una lista de resultados con un formato parecido al siguiente:</p><br />
<blockquote> archivo1.txt<br /> archivo2.txt<br /> ...<br /> archivoN.txt<br /> [espacio]<br /> archivo1.dat<br /> archivo2.dat<br /> [espacio]<br /> </blockquote>
<p>Cada grupo de archivos (los que no se separan por una línea en blanco) son archivos duplicados. Cuantos más archivos en cada bloque, más duplicaciones.  </p>
<p>La tarea sucia está hecha y ahora nos queda a nosotros hacer una parte de tarea manual.  Para ello será necesario abrir el archivo duplicados.txt y dejar sólo los archivos que queremos eliminar.  Es decir, debemos borrar de la lista de archivo todas aquellas entradas que queremos conservar.</p>
<p>Una vez hecho esto, sólo nos queda invocar a nuestro querido awk para que convierta cada entrada en una línea de comando del shell:</p><br />
<blockquote># awk &#39;{gsub(/ |\(|\)/,"\\\&amp;");print "rm "$0}&#39; < duplicados.txt | sh </blockquote>
<p>Es decir, para cada línea del archivo duplicados.txt (< duplicados.txt) creo una línea que empiece por <em>rm </em>e incluya todo el contenido de la línea (print "rm "$0), enviando el resultado a ejecutar al shell (... | sh).</p>
<p>Con esto, se ejecuta línea por línea todos los archivos a eliminar.   El comando gsub "escapa" los caracteres especiales que el shell podría malinterpretar como parte del comando a ejecutar y no como parte del nombre del archivo.  Por ejemplo, los nombres que incluyen paréntesis o los que incluyan espacios, incluirán contrabarras para evitar ambig&uuml;edades al shell.</p>
<p>Con este proceso, que en mi caso, con 150.000 archivos (muchos peque&ntilde;os) duró unas dos horas entre fdupes, procesado manual del archivo y limpieza final, hemos eliminado los archivos duplicados.</p><br />
<h2>Duplicados en sistemas windows</h2>
<p>Como ya he dicho antes, mi testeo en sistemas Windows ha sido muy limitado, por lo que os refiero al <a href="http://articles.techrepublic.com.com/5100-10877_11-6160661.html" title="Techrepublic: Remove clutter with Windows XP SP2 Dupfinder">artículo de Techrepublic sobre limpieza de duplicados</a> que habla sobre la instalación y ejecución del programa que he testeado: Dupfinder.  </p>
<p>Esta aplicación funciona, cómo no, con interficie gráfica.  Eso lo hace mucho más amena para los usuarios no muy dados a la línea de comandos.  El funcionamiento es esencialmente el mismo, aunque es imposible decir si el criterio utilizado para detectar si dos archivos son duplicados es exactamente el mismo o no que en casos como fdupes.</p><br />
<h2>Conclusiones </h2>
<p>Hasta aquí la primera parte de mis tareas de reorganización de la información personal. Espero que te haya servido de ayuda toda esta información. En mi caso pude liberar un 10% de mi espacio en disco (es mucho, principalmente debido a copias de seguridad no comprimidas y archivos de gran tama&ntilde;o), aunque lo más importante es que esto me permite reducir tiempo para las tareas posteriores de reorganización de los archivos existentes. </p>
<p>En mi opinión, lo importante es cargarse de paciencia e ir avanzando. Si no puedes cerrar la limpieza en un solo día, no pasa nada: continúa en otro momento.  Lo importante es que al acabar tengas los archivos necesarios, ni más ni menos. </p></p>
